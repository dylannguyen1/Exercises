{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task of data engineer\n",
    "Set up scheduled ingestion of data from the application databases to an analytical database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Engineer:\n",
    "    - Cloud technology\n",
    "    - Develop Scalable Data Architecture\n",
    "    - Streamline Data Acquisition\n",
    "    - Set up processes to bring together data\n",
    "    - Clear Corrupted Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Scientist: \n",
    "    - Mining data for pattern\n",
    "    - Apply statistical models on large datasets\n",
    "    - Build predictive models using ML\n",
    "    - Develop tools to monitor business process\n",
    "    - Clean outliers in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Engineer Problem:\n",
    "\n",
    "###Data scientists are querying the online store databases directly and slowing down the functioning of the application since it's using the same database. ###\n",
    "Data Engineer should make sure there is a seperate database for analytic\n",
    "\n",
    "###The online store is slow because the application's database server doesn't have enough memory.### infrastructure problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools of data engineer\n",
    "#Processing example\n",
    "##cluster of computers perform these operations using PySpark framework\n",
    "\n",
    "df = spark.read.parquet(\"users.parquet\") \n",
    "\n",
    "outliers = df.filter(df[\"age\"] > 100)\n",
    "\n",
    "print(outliers.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing tasks\n",
    "- Data engineers often have to join, clean, or organize data before loading it into a destination analytics database. This is done in the data processing, or data transformation step.\n",
    "- Data Processing is distributed over clusters of virtual machines(e.g. using Spark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scheduling tools\n",
    "\n",
    "Make sure jobs run in a specific order and all dependencies are resolved correctly.\n",
    "\n",
    "Make sure the jobs run at midnight UTC each day.\n",
    "\n",
    "###Scale up the number of nodes when there's lots of data to be processed.### Jobs of processing tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why Cloud computing ?\n",
    "\n",
    "The cloud can provide you with the resources you need, when you need them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Data engineering toolbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Databases: A usually large collection of data organized especially for rapid search and retrieval\n",
    "\n",
    "- Holds data\n",
    "- Organise data\n",
    "- Retrieve/ Search Data\n",
    "Database management system- \n",
    "DBMS -much more organised than file systems\n",
    "- functitons: search, replication\n",
    "Structured vs unstructured data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL database schema:\n",
    "    \n",
    "## Create Customer table\n",
    "    CREATE TABLE \"Customer\"(\n",
    "    \"id\" SERIAL not NULL,\n",
    "    \"first_name\" varchar,\n",
    "    \"second_name\" varchar,\n",
    "    PRIMARY KEY (\"id\")\n",
    "    )\n",
    "    \n",
    "## Create order table\n",
    "    CREATE TABLE \"Order\"(\n",
    "    \"id\" SERIAL not NULL,\n",
    "    \"customer_id\" integer REFERENCES \" Customer\",\n",
    "    \"product_name\" varchar,\n",
    "    \"product_price\" integer,\n",
    "        \n",
    "    PRIMARY KEY(\"id\")\n",
    "    )\n",
    "\n",
    "SELECT * FROM \"Customer\"\n",
    "INNER JOIN \"Order\"\n",
    "ON \"customer_id\" = \"Customer\" .\"id\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL:\n",
    "    - Customer data in a store's database\n",
    "    - Always has a database schema\n",
    "    - MySQL, PostgreSQL\n",
    "    \n",
    "NoSQL:\n",
    "    - can be schemaless\n",
    "    - Key- value stores: Redis e.g. catching layer in distributed web server\n",
    "    - MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The database schema\n",
    "\n",
    "#db_engine: database engine, which has been defined for you and is called db_engine\n",
    "#db_enginer = Engine(postgresql://repl@/postgres)    \n",
    "\n",
    "#Complete the SELECT statement\n",
    "\n",
    "data = pd.read_sql(\"\"\"\n",
    "SELECT first_name, last_name FROM \"Customer\"\n",
    "ORDER BY last_name, first_name\n",
    "\"\"\", db_engine)\n",
    "\n",
    "#Show the first 3 rows of the DataFrame\n",
    "\n",
    "print(data.head(n=3))\n",
    "\n",
    "#Show the info of the DataFrame\n",
    "\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Complete the SELECT statement\n",
    "\n",
    "data = pd.read_sql(\"\"\"\n",
    "\n",
    "SELECT * FROM \"Customer\"\n",
    "\n",
    "INNER JOIN \"Order\"\n",
    "\n",
    "ON \"Order\".\"customer_id\"=\"Customer\".\"id\"\n",
    "\n",
    "\"\"\", db_engine)\n",
    "\n",
    "#Show the id column of data\n",
    "print(data.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parallel computing\n",
    "Parallel computing can optimize the use of multiple processing units.\n",
    "Parallel computing can optimize the use of memory between several machines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiprocessing.Pool\n",
    "from multiprocessing import Tool\n",
    "## function take_mean_Age\n",
    "def take_mean_age(year_and_group):\n",
    "    year, group = year_and_group\n",
    "    return pd.DataFrame({\"Age\":group['Age'].mean()}, index = [year])\n",
    "\n",
    "with Pool(4) as p: ##using 4 cores\n",
    "    results = p.map(take_mean_age, athlete_events.groupby(\"Year\"))\n",
    "    \n",
    "## concatenate the results to form the resulting DataFrame\n",
    "result_df = pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dask framework to avoid write low-level code\n",
    "import dask.dataframe as dd\n",
    "\n",
    "## partition dataframe into 4\n",
    "athlete_events_dask = dd.from_pandas(athlete_events, npartitions = 4)\n",
    "\n",
    "## Run parallel computations from each partition\n",
    "result_df = athlete_events_dask.groupby(\"Year\").Age.mean().compute() ## dask use lazy evaluation hence add.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From task to subtasks\n",
    "# Function to apply a function over multiple cores\n",
    "@print_timing\n",
    "#It takes in as input the function being applied, the grouping used, and the number of cores needed for the analysis.\n",
    "def parallel_apply(apply_func, groups, nb_cores):\n",
    "    with Pool(nb_cores) as p:\n",
    "        results = p.map(apply_func, groups)\n",
    "    return pd.concat(results)\n",
    "\n",
    "# Parallel apply using 1 core\n",
    "parallel_apply(take_mean_age, athlete_events.groupby('Year'), nb_cores = 1)\n",
    "\n",
    "# Parallel apply using 2 cores\n",
    "parallel_apply(take_mean_age, athlete_events.groupby('Year'), nb_cores = 2)\n",
    "\n",
    "# Parallel apply using 4 cores\n",
    "parallel_apply(take_mean_age, athlete_events.groupby('Year'), nb_cores = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Team</th>\n",
       "      <th>NOC</th>\n",
       "      <th>Games</th>\n",
       "      <th>Year</th>\n",
       "      <th>Season</th>\n",
       "      <th>City</th>\n",
       "      <th>Sport</th>\n",
       "      <th>Event</th>\n",
       "      <th>Medal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>A Dijiang</td>\n",
       "      <td>M</td>\n",
       "      <td>24.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>China</td>\n",
       "      <td>CHN</td>\n",
       "      <td>1992 Summer</td>\n",
       "      <td>1992</td>\n",
       "      <td>Summer</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Basketball</td>\n",
       "      <td>Basketball Men's Basketball</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>A Lamusi</td>\n",
       "      <td>M</td>\n",
       "      <td>23.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>China</td>\n",
       "      <td>CHN</td>\n",
       "      <td>2012 Summer</td>\n",
       "      <td>2012</td>\n",
       "      <td>Summer</td>\n",
       "      <td>London</td>\n",
       "      <td>Judo</td>\n",
       "      <td>Judo Men's Extra-Lightweight</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Gunnar Nielsen Aaby</td>\n",
       "      <td>M</td>\n",
       "      <td>24.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>DEN</td>\n",
       "      <td>1920 Summer</td>\n",
       "      <td>1920</td>\n",
       "      <td>Summer</td>\n",
       "      <td>Antwerpen</td>\n",
       "      <td>Football</td>\n",
       "      <td>Football Men's Football</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Edgar Lindenau Aabye</td>\n",
       "      <td>M</td>\n",
       "      <td>34.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Denmark/Sweden</td>\n",
       "      <td>DEN</td>\n",
       "      <td>1900 Summer</td>\n",
       "      <td>1900</td>\n",
       "      <td>Summer</td>\n",
       "      <td>Paris</td>\n",
       "      <td>Tug-Of-War</td>\n",
       "      <td>Tug-Of-War Men's Tug-Of-War</td>\n",
       "      <td>Gold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Christine Jacoba Aaftink</td>\n",
       "      <td>F</td>\n",
       "      <td>21.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>NED</td>\n",
       "      <td>1988 Winter</td>\n",
       "      <td>1988</td>\n",
       "      <td>Winter</td>\n",
       "      <td>Calgary</td>\n",
       "      <td>Speed Skating</td>\n",
       "      <td>Speed Skating Women's 500 metres</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                      Name Sex   Age  Height  Weight            Team  \\\n",
       "0   1                 A Dijiang   M  24.0   180.0    80.0           China   \n",
       "1   2                  A Lamusi   M  23.0   170.0    60.0           China   \n",
       "2   3       Gunnar Nielsen Aaby   M  24.0     NaN     NaN         Denmark   \n",
       "3   4      Edgar Lindenau Aabye   M  34.0     NaN     NaN  Denmark/Sweden   \n",
       "4   5  Christine Jacoba Aaftink   F  21.0   185.0    82.0     Netherlands   \n",
       "\n",
       "   NOC        Games  Year  Season       City          Sport  \\\n",
       "0  CHN  1992 Summer  1992  Summer  Barcelona     Basketball   \n",
       "1  CHN  2012 Summer  2012  Summer     London           Judo   \n",
       "2  DEN  1920 Summer  1920  Summer  Antwerpen       Football   \n",
       "3  DEN  1900 Summer  1900  Summer      Paris     Tug-Of-War   \n",
       "4  NED  1988 Winter  1988  Winter    Calgary  Speed Skating   \n",
       "\n",
       "                              Event Medal  \n",
       "0       Basketball Men's Basketball   NaN  \n",
       "1      Judo Men's Extra-Lightweight   NaN  \n",
       "2           Football Men's Football   NaN  \n",
       "3       Tug-Of-War Men's Tug-Of-War  Gold  \n",
       "4  Speed Skating Women's 500 metres   NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "athlete_events = pd.read_csv(\"athlete_events.csv\")\n",
    "athlete_events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using a dataframe \n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Set the number of pratitions\n",
    "athlete_events_dask = dd.from_pandas(athlete_events, npartitions = 4)\n",
    "\n",
    "# Calculate the mean Age per Year\n",
    "print(athlete_events_dask.groupby('Year').Age.mean().compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parallel computation framework\n",
    "# Hive Example\n",
    "# Select average age of athlete per years they participate\n",
    "SELECT year,AVG(age)\n",
    "FROM views.athlete_events\n",
    "GROUP BY year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PySpark an example\n",
    "## Load the dataset into athlete_Events_spark first\n",
    "(athlete_events_spark\n",
    "    .groupBy(\"Year\")\n",
    "    .mean(\"Age\")\n",
    "    .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop\n",
    "- MapReduce is a part of it\n",
    "- Collection of open-source packages for Big Data\n",
    "- HDFS is a part of it\n",
    "\n",
    "PySpark\n",
    "- Uses DataFrame abstraction\n",
    "- Python interface for the Spark framework\n",
    "\n",
    "Hive\n",
    "- Initially used Hadoop MapReduce\n",
    "- is built from the need to use structure queries for parallel processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".printSchema(): helps print the schema of a Spark DataFrame.\n",
    ".groupBy(): grouping statement for an aggregation.\n",
    ".mean(): take the mean over each group.\n",
    ".show(): show the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A PySpark groupby\n",
    "\n",
    "# Print the type of athlete_events_spark\n",
    "print(type(athlete_events_spark))\n",
    "\n",
    "# Print the schema of athlete_events_spark\n",
    "print(athlete_events_spark.printSchema())\n",
    "\n",
    "# Group by the Year, and find the mean Age\n",
    "print(athlete_events_spark.groupBy('Year').mean(\"Age\"))\n",
    "\n",
    "# Group by the Year, and find the mean Age\n",
    "print(athlete_events_spark.groupBy('Year').mean(\"Age\").show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat /home/repl/spark-script.py\n",
    "\n",
    "spark-submit \\\n",
    "  --master local[4] \\\n",
    "  /home/repl/spark-script.py\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2018/11/data-engineer-comprehensive-list-resources-get-started/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create DAG object\n",
    "dag = DAG(dag_id = \"example_dag\", ..., schedule_interval = \"0 * * * *\")\n",
    "# Define operations\n",
    "start_cluster = StartClusterOperator(task_id = \"start_cluster\", dag = dag)\n",
    "ingest_customer_data = SparkJobOperator(task_id = \"ingest_customer_data\", dag =dag)\n",
    "ingest_product_data = SparkJobOperator(task_id = \"ingest_product_data\", dag =dag)\n",
    "enrich_customer_data = PythonOperator (task_id = \"enrich_customer_data\", dag =dag)\n",
    "\n",
    "# Set up dependency flow\n",
    "start_cluster.set_downstream(ingest_customer_data)\n",
    "ingest_customer_data.set_downstream(enrich_customer_data)\n",
    "ingest_product_data.set_downstream(enrich_customer_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DAG object\n",
    "dag = DAG(dag_id=\"car_factory_simulation\",\n",
    "          default_args={\"owner\": \"airflow\",\"start_date\": airflow.utils.dates.days_ago(2)},\n",
    "          schedule_interval=\"0 * * * *\") ## run everyhour at minutes 0\n",
    "\n",
    "# Task definitions\n",
    "assemble_frame = BashOperator(task_id=\"assemble_frame\", bash_command='echo \"Assembling frame\"', dag=dag)\n",
    "place_tires = BashOperator(task_id=\"place_tires\", bash_command='echo \"Placing tires\"', dag=dag)\n",
    "assemble_body = BashOperator(task_id=\"assemble_body\", bash_command='echo \"Assembling body\"', dag=dag)\n",
    "apply_paint = BashOperator(task_id=\"apply_paint\", bash_command='echo \"Applying paint\"', dag=dag)\n",
    "\n",
    "# Complete the downstream flow\n",
    "assemble_frame.set_downstream(place_tires)\n",
    "assemble_frame.set_downstream(assemble_body)\n",
    "assemble_body.set_downstream(apply_paint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3. Extract transform load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value_1\n"
     ]
    }
   ],
   "source": [
    "#JSON\n",
    "import json\n",
    "result = json.loads('{\"key_1\" : \"value_1\",\"key_2\" :\"value_2\"}')\n",
    "\n",
    "print(result[\"key_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'by': 'neis', 'descendants': 0, 'id': 16222426, 'score': 17, 'time': 1516800333, 'title': 'Duolingo-Style Learning for Data Science: DataCamp for Mobile', 'type': 'story', 'url': 'https://medium.com/datacamp/duolingo-style-learning-for-data-science-datacamp-for-mobile-3861d1bc02df'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "response = requests.get(\"https://hacker-news.firebaseio.com/v0/item/16222426.json\")\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extraction from databases\n",
    "import sqlalchemy\n",
    "connection_url = \"postgresql://repl:password#localhost:5432/pagila\"\n",
    "db_enginer = sqlalchemy.create_engine(connection_url)\n",
    "## interact with the data\n",
    "import pandas as pd\n",
    "pd.read_sql(\"SELECT * FROM customer\",db_enginer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'by': 'neis', 'descendants': 0, 'id': 16222426, 'score': 17, 'time': 1516800333, 'title': 'Duolingo-Style Learning for Data Science: DataCamp for Mobile', 'type': 'story', 'url': 'https://medium.com/datacamp/duolingo-style-learning-for-data-science-datacamp-for-mobile-3861d1bc02df'}\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "##Fetch from an API\n",
    "\n",
    "import requests\n",
    "\n",
    "# Fetch the Hackernews post\n",
    "resp = requests.get(\"https://hacker-news.firebaseio.com/v0/item/16222426.json\")\n",
    "\n",
    "# Print the response parsed as JSON\n",
    "print(resp.json())\n",
    "\n",
    "# Assign the score of the test to post_score\n",
    "post_score = resp.json()[\"score\"]\n",
    "print(post_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read from a database\n",
    "import pandas as pd\n",
    "# Function to extract table to a pandas DataFrame\n",
    "def extract_table_to_pandas(tablename, db_engine):\n",
    "    query = \"SELECT * FROM {}\".format(tablename)\n",
    "    return pd.read_sql(query, db_engine)\n",
    "\n",
    "# Connect to the database using the connection URI\n",
    "connection_uri = \"postgresql://repl:password@localhost:5432/pagila\" \n",
    "db_engine = sqlalchemy.create_engine(connection_uri)\n",
    "\n",
    "# Extract the film table into a pandas DataFrame\n",
    "extract_table_to_pandas(\"film\", db_engine)\n",
    "\n",
    "# Extract the customer table into a pandas DataFrame\n",
    "extract_table_to_pandas(\"customer\", db_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform\n",
    "\n",
    "customer_df ## pandas frame with customer data\n",
    "# Split email column into 2 columns on the @ symbol, first column everything before @, second one everything after @\n",
    "split_email = customer_df.email.str.split(\"@\", expand = True)\n",
    "\n",
    "## Create 2 new columns using resulting DataFrame\n",
    "customer_df = customer_df.assign(\n",
    "    username = split_email[0],\n",
    "    domain = split_email[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"C:/Program\\ Files/Java/jdk1.8.0_60\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.jdbc(\"jdbc:postgresql://localhost:5432/pagila\",\n",
    "                \"customer\", #the name of the table\n",
    "               properties = {\"user\" : \"repl\", \"password\" : \"password\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Join example using PySpark\n",
    "customer_df ## PySpark DataFrame with customer data\n",
    "ratings_df ## PySpark DataFrame with ratings data\n",
    "\n",
    "## get the mean rating of each customer and add it to the dataframe\n",
    "\n",
    "# groupby customer id then calculate mean\n",
    "ratings_per_customer = ratings_df.groupBy(\"customer_id\").mean(\"rating\")\n",
    "\n",
    "# Join on customer ID\n",
    "customer_df.join(ratings_per_customer, \n",
    "                 customer_df.customer_id == ratings_per_customer.customer_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
